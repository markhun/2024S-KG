// Cypher data import script generated by Neo4j data importer

:param {
  // Define the file path root and the individual file names required for loading.
  // https://neo4j.com/docs/operations-manual/current/configuration/file-locations/
  file_path_root: 'file:///', // Change this to the folder your script can access the files at.
  file_0: 'ratings.csv',
  file_1: 'tags_reduced.csv',
  file_2: 'books_enriched.csv',
  file_3: 'to_read.csv',
  file_4: 'book_tags_reduced.csv'
};

// CONSTRAINT creation
// -------------------
//
// Create node uniqueness constraints, ensuring no duplicates for the given node label and ID property exist in the database. This also ensures no duplicates are introduced in future.
//
// NOTE: The following constraint creation syntax is generated based on the current connected database version 5.20.0.
CREATE CONSTRAINT `book_id_Book_uniq` IF NOT EXISTS
FOR (n: `Book`)
REQUIRE (n.`book_id`) IS UNIQUE;
CREATE CONSTRAINT `author_Author_uniq` IF NOT EXISTS
FOR (n: `Author`)
REQUIRE (n.`author`) IS UNIQUE;
CREATE CONSTRAINT `tag_id_Tag_uniq` IF NOT EXISTS
FOR (n: `Tag`)
REQUIRE (n.`tag_id`) IS UNIQUE;
CREATE CONSTRAINT `user_id_User_uniq` IF NOT EXISTS
FOR (n: `User`)
REQUIRE (n.`user_id`) IS UNIQUE;
CREATE CONSTRAINT `name_Bookseries_uniq` IF NOT EXISTS
FOR (n: `Bookseries`)
REQUIRE (n.`name`) IS UNIQUE;
CREATE CONSTRAINT `genre_Genre_uniq` IF NOT EXISTS
FOR (n: `Genre`)
REQUIRE (n.`genre`) IS UNIQUE;

:param {
  idsToSkip: []
};

// NODE load
// ---------
//
// Load nodes in batches, one node label at a time. Nodes will be created using a MERGE statement to ensure a node with the same label and ID property remains unique. Pre-existing nodes found by a MERGE statement will have their other properties set to the latest values encountered in a load file.
//
// NOTE: Any nodes with IDs in the 'idsToSkip' list parameter will not be loaded.
LOAD CSV WITH HEADERS FROM ($file_path_root + $file_2) AS row
WITH row
WHERE NOT row.`book_id` IN $idsToSkip AND NOT toInteger(trim(row.`book_id`)) IS NULL
CALL {
  WITH row
  MERGE (n: `Book` { `book_id`: toInteger(trim(row.`book_id`)) })
  SET n.`book_id` = toInteger(trim(row.`book_id`))
  SET n.`goodreads_work_id` = toInteger(trim(row.`goodreads_book_id`))
  SET n.`isbn` = row.`isbn`
  SET n.`isbn13` = row.`isbn13`
  SET n.`original_title` = row.`original_title`
  SET n.`title` = row.`title`
  SET n.`language_code` = row.`language_code`
  SET n.`average_rating` = toFloat(trim(row.`average_rating`))
  SET n.`ratings_count` = toInteger(trim(row.`ratings_count`))
  SET n.`image_url` = row.`image_url`
  SET n.`publication_year` = toInteger(trim(row.`original_publication_year`))
  SET n.`goodreads_book_id` = toInteger(trim(row.`goodreads_book_id`))
  SET n.`dbpedia_resource` = row.`dbpedia_resource`
  SET n.`work_ratings_count` = toInteger(trim(row.`work_ratings_count`))
  SET n.`work_text_reviews_count` = toInteger(trim(row.`work_text_reviews_count`))
  SET n.`ratings_1` = toInteger(trim(row.`ratings_1`))
  SET n.`ratings_2` = toInteger(trim(row.`ratings_2`))
  SET n.`ratings_3` = toInteger(trim(row.`ratings_3`))
  SET n.`ratings_4` = toInteger(trim(row.`ratings_4`))
  SET n.`ratings_5` = toInteger(trim(row.`ratings_5`))
} IN TRANSACTIONS OF 10000 ROWS;

LOAD CSV WITH HEADERS FROM ($file_path_root + $file_2) AS row
WITH row
WHERE NOT row.`authors` IN $idsToSkip AND NOT row.`authors` IS NULL
CALL {
  WITH row
  MERGE (n: `Author` { `author`: row.`authors` })
  SET n.`author` = row.`authors`
} IN TRANSACTIONS OF 10000 ROWS;

LOAD CSV WITH HEADERS FROM ($file_path_root + $file_1) AS row
WITH row
WHERE NOT row.`tag_id` IN $idsToSkip AND NOT toInteger(trim(row.`tag_id`)) IS NULL
CALL {
  WITH row
  MERGE (n: `Tag` { `tag_id`: toInteger(trim(row.`tag_id`)) })
  SET n.`tag_id` = toInteger(trim(row.`tag_id`))
  SET n.`name` = row.`tag_name`
} IN TRANSACTIONS OF 10000 ROWS;

LOAD CSV WITH HEADERS FROM ($file_path_root + $file_0) AS row
WITH row
WHERE NOT row.`user_id` IN $idsToSkip AND NOT toInteger(trim(row.`user_id`)) IS NULL
CALL {
  WITH row
  MERGE (n: `User` { `user_id`: toInteger(trim(row.`user_id`)) })
  SET n.`user_id` = toInteger(trim(row.`user_id`))
} IN TRANSACTIONS OF 10000 ROWS;

LOAD CSV WITH HEADERS FROM ($file_path_root + $file_2) AS row
WITH row
WHERE NOT row.`book_series` IN $idsToSkip AND NOT row.`book_series` IS NULL
CALL {
  WITH row
  MERGE (n: `Bookseries` { `name`: row.`book_series` })
  SET n.`name` = row.`book_series`
} IN TRANSACTIONS OF 10000 ROWS;

LOAD CSV WITH HEADERS FROM ($file_path_root + $file_2) AS row
WITH row
WHERE NOT row.`genres` IN $idsToSkip AND NOT row.`genres` IS NULL
CALL {
  WITH row
  MERGE (n: `Genre` { `genre`: row.`genres` })
  SET n.`genre` = row.`genres`
} IN TRANSACTIONS OF 10000 ROWS;


// RELATIONSHIP load
// -----------------
//
// Load relationships in batches, one relationship type at a time. Relationships are created using a MERGE statement, meaning only one relationship of a given type will ever be created between a pair of nodes.
LOAD CSV WITH HEADERS FROM ($file_path_root + $file_2) AS row
WITH row 
CALL {
  WITH row
  MATCH (source: `Author` { `author`: row.`authors` })
  MATCH (target: `Book` { `book_id`: toInteger(trim(row.`book_id`)) })
  MERGE (source)-[r: `WROTE`]->(target)
} IN TRANSACTIONS OF 10000 ROWS;

LOAD CSV WITH HEADERS FROM ($file_path_root + $file_0) AS row
WITH row 
CALL {
  WITH row
  MATCH (source: `User` { `user_id`: toInteger(trim(row.`user_id`)) })
  MATCH (target: `Book` { `book_id`: toInteger(trim(row.`book_id`)) })
  MERGE (source)-[r: `RATED`]->(target)
  SET r.`rating` = toInteger(trim(row.`rating`))
} IN TRANSACTIONS OF 10000 ROWS;

LOAD CSV WITH HEADERS FROM ($file_path_root + $file_3) AS row
WITH row 
CALL {
  WITH row
  MATCH (source: `User` { `user_id`: toInteger(trim(row.`user_id`)) })
  MATCH (target: `Book` { `book_id`: toInteger(trim(row.`book_id`)) })
  MERGE (source)-[r: `INTERESTED`]->(target)
} IN TRANSACTIONS OF 10000 ROWS;

LOAD CSV WITH HEADERS FROM ($file_path_root + $file_4) AS row
WITH row 
CALL {
  WITH row
  MATCH (source: `Book` { `book_id`: toInteger(trim(row.`book_id`)) })
  MATCH (target: `Tag` { `tag_id`: toInteger(trim(row.`tag_id`)) })
  MERGE (source)-[r: `TAGGED`]->(target)
  SET r.`count` = toInteger(trim(row.`count`))
} IN TRANSACTIONS OF 10000 ROWS;

LOAD CSV WITH HEADERS FROM ($file_path_root + $file_2) AS row
WITH row 
CALL {
  WITH row
  MATCH (source: `Bookseries` { `name`: row.`book_series` })
  MATCH (target: `Book` { `book_id`: toInteger(trim(row.`book_id`)) })
  MERGE (source)-[r: `INCLUDES`]->(target)
} IN TRANSACTIONS OF 10000 ROWS;

LOAD CSV WITH HEADERS FROM ($file_path_root + $file_2) AS row
WITH row 
CALL {
  WITH row
  MATCH (source: `Book` { `book_id`: toInteger(trim(row.`book_id`)) })
  MATCH (target: `Genre` { `genre`: row.`genres` })
  MERGE (source)-[r: `hasGENRE`]->(target)
} IN TRANSACTIONS OF 10000 ROWS;
